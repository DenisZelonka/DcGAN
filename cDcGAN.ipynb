{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zelon\\AppData\\Roaming\\Python\\Python39\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torchvision import datasets,transforms\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torchvision.utils as vutils\n",
    "import ignite\n",
    "from ignite.engine import Engine, Events\n",
    "import ignite.distributed as idist\n",
    "import logging\n",
    "from ignite.metrics import FID, InceptionScore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading up loggers, setting seed for reproducibility, using ignite for more readable code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Logger ignite.distributed.launcher.Parallel (WARNING)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ignite.utils.manual_seed(999) ##42 nefunguje\n",
    "ignite.utils.setup_logger(name=\"ignite.distributed.auto.auto_dataloader\", level=logging.WARNING)\n",
    "ignite.utils.setup_logger(name=\"ignite.distributed.launcher.Parallel\", level=logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting parameters, source: https://arxiv.org/pdf/1511.06434.pdf%C3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "learning_rate=0.0002 #0.0002\n",
    "epochs=100\n",
    "z=4096\n",
    "in_channels=3\n",
    "img_shape=64\n",
    "cat=2\n",
    "path_to_progress_img=\"./progress/\"\n",
    "path_to_weights=\"./weights/\"\n",
    "path_to_images=\"./img/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "dataset=datasets.ImageFolder(\n",
    "    path_to_images,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize((64,64)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5)),\n",
    "    ]))\n",
    "test_dataset = torch.utils.data.Subset(dataset, numpy.random.choice(len(dataset), 200, replace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = idist.auto_dataloader(dataset, batch_size=batch_size,\n",
    "                                         shuffle=True,num_workers=2)\n",
    "dataloader_test = idist.auto_dataloader(test_dataset, batch_size=batch_size,\n",
    "                                         shuffle=False,num_workers=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://arxiv.org/pdf/1511.06434.pdf%C3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self,labels,embedding_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.convT = nn.Sequential(\n",
    "        nn.ConvTranspose2d(in_channels=z+embedding_dim**2,out_channels=512,kernel_size=4,stride=1,padding=0),\n",
    "        nn.BatchNorm2d(512),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        nn.ConvTranspose2d(in_channels=512,out_channels=256,kernel_size=4,stride=2,padding=1),\n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        nn.ConvTranspose2d(in_channels=256,out_channels=128,kernel_size=4,stride=2,padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        nn.ConvTranspose2d(in_channels=128,out_channels=64,kernel_size=4,stride=2,padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        nn.ConvTranspose2d(in_channels=64,out_channels=in_channels,kernel_size=4,stride=2,padding=1),\n",
    "        nn.Tanh(),\n",
    "        )\n",
    "        self.embedding=nn.Embedding(labels,embedding_dim**2)\n",
    "        self.embedding_dim=embedding_dim\n",
    "    def forward(self, latent,label):\n",
    "        lat_emb_vector=torch.cat([latent,self.embedding(label).reshape(label.shape[0],self.embedding_dim**2,1,1)],1)\n",
    "        img=self.convT(lat_emb_vector)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,labels,embedding_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels+1,out_channels=64,kernel_size=4,stride=2,padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(in_channels=64,out_channels=128,kernel_size=4,stride=2,padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(in_channels=128,out_channels=256,kernel_size=4,stride=2,padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(in_channels=256,out_channels=512,kernel_size=4,stride=2,padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(in_channels=512,out_channels=1,kernel_size=4,stride=2,padding=0),\n",
    "        )\n",
    "        self.embedding=nn.Embedding(labels,embedding_dim**2)\n",
    "        self.embedding_dim=embedding_dim\n",
    "    def forward(self, img,label):\n",
    "        label = self.embedding(label).view(-1, 1, img_shape, img_shape)\n",
    "        img_emb_vector = torch.cat([img,label], 1)\n",
    "        valid=self.conv(img_emb_vector)\n",
    "        return valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen=idist.auto_model(Generator(cat,img_shape))\n",
    "dis=idist.auto_model(Discriminator(cat,img_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossFun=nn.BCEWithLogitsLoss()\n",
    "fixed_noise = torch.randn(8, z, 1, 1)\n",
    "fixed_labels=torch.randint(low=0, high=cat,size=(8,1,1,1))\n",
    "real_label = 1\n",
    "fake_label = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizerD = torch.optim.Adam(dis.parameters(), lr=learning_rate,betas=(0.5, 0.999))\n",
    "optimizerG = torch.optim.Adam(gen.parameters(), lr=learning_rate,betas=(0.5, 0.999))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://machinelearningmastery.com/how-to-code-generative-adversarial-network-hacks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_positive_labels(y):\n",
    " return y - 0.3 + (torch.rand(y.shape,device=device) * 0.5)\n",
    "def smooth_negative_labels(y):\n",
    " return y + torch.rand(y.shape,device=device) * 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(engine,data):\n",
    "                dis.zero_grad()\n",
    "                real=data[0].to(device)\n",
    "                batch_size=real.size(0)\n",
    "                labelTrue = (torch.full((batch_size,1,1,1), real_label,\n",
    "                       dtype=real.dtype, device=device))\n",
    "                labelFalse = (torch.full((batch_size,1,1,1), fake_label,\n",
    "                       dtype=real.dtype, device=device))\n",
    "                disReal = dis(real,data[1])\n",
    "                lossReal=lossFun(disReal,labelTrue)\n",
    "                lossReal.backward()\n",
    "                D_x = disReal.mean().item()\n",
    "\n",
    "                noise = torch.randn(batch_size, z, 1, 1, device=device)\n",
    "                gen_label=torch.randint(low=0, high=cat,size=(batch_size,1,1,1), device=device)\n",
    "                fake = gen(noise,gen_label)\n",
    "                disFakeDisBack = dis(fake.detach(),gen_label)\n",
    "                errD_fake = lossFun(disFakeDisBack, labelFalse)\n",
    "                errD = lossReal + errD_fake\n",
    "                errD_fake.backward()\n",
    "                D_G_z1 = disFakeDisBack.mean().item()\n",
    "                optimizerD.step()\n",
    "\n",
    "                gen.zero_grad()\n",
    "                disFakeGenBack = dis(fake,gen_label)\n",
    "                errG = lossFun(disFakeGenBack, labelTrue)\n",
    "                errG.backward()\n",
    "                D_G_z2 = disFakeGenBack.mean().item()\n",
    "                optimizerG.step()\n",
    "\n",
    "                \n",
    "                return {\n",
    "                \"Loss_G\" : errG.item(),\n",
    "                \"Loss_D\" : errD.item(),\n",
    "                \"D_x\": D_x,\n",
    "                \"D_G_z1\": D_G_z1,\n",
    "                \"D_G_z2\": D_G_z2,\n",
    "                }\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://pytorch-ignite.ai/blog/gan-evaluation-with-fid-and-is/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Engine(training_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_fn(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@trainer.on(Events.STARTED)\n",
    "def init_weights():\n",
    "    if (len(os.listdir(path_to_weights)) == 0):\n",
    "        dis.apply(initialize_fn)\n",
    "        gen.apply(initialize_fn)\n",
    "    else:\n",
    "        idx=len(os.listdir(path_to_weights))//2\n",
    "        dis.load_state_dict(torch.load(path_to_weights+\"dis_epoch_%d.pth\" % idx))\n",
    "        gen.load_state_dict(torch.load(path_to_weights+\"gen_epoch_%d.pth\" % idx))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_losses = []\n",
    "D_losses = []\n",
    "D_x = []\n",
    "D_G_z1 = []\n",
    "D_G_z2 = []\n",
    "\n",
    "\n",
    "\n",
    "@trainer.on(Events.ITERATION_COMPLETED)\n",
    "def store_losses(engine):\n",
    "    o = engine.state.output\n",
    "    G_losses.append(o[\"Loss_G\"])\n",
    "    D_losses.append(o[\"Loss_D\"])\n",
    "    D_x.append(o[\"D_x\"])\n",
    "    D_G_z1.append(o[\"D_G_z1\"])\n",
    "    D_G_z2.append(o[\"D_G_z2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@trainer.on(Events.ITERATION_COMPLETED(every=100))\n",
    "def store_images(engine):\n",
    "    with torch.no_grad():\n",
    "        fake = gen(fixed_noise,fixed_labels).cpu()\n",
    "    vutils.save_image(fake.detach(),\n",
    "            '%s/fake_samples_epoch_%03d.png' % (path_to_progress_img, engine.state.epoch),\n",
    "            normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid_metric = FID(device=idist.device())\n",
    "is_metric = InceptionScore(device=idist.device(),output_transform=lambda x: x[0]) #, num_features=11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image as Image\n",
    "\n",
    "\n",
    "def interpolate(batch):\n",
    "    arr = []\n",
    "    for img in batch:\n",
    "        pil_img = transforms.ToPILImage()(img)\n",
    "        resized_img = pil_img.resize((299,299), Image.BILINEAR)\n",
    "        #resized_img=resized_img.convert('RGB')\n",
    "        arr.append(transforms.ToTensor()(resized_img))\n",
    "        \n",
    "    return torch.stack(arr)\n",
    "\n",
    "\n",
    "def evaluation_step(engine, batch):\n",
    "    with torch.no_grad():\n",
    "        noise = torch.randn(len(batch[\"image\"]), z, 1, 1, device=idist.device())\n",
    "        gen.eval()\n",
    "        fake_batch = gen(noise,batch[\"label\"])\n",
    "        fake = interpolate(fake_batch)\n",
    "        real = interpolate(batch[\"image\"])\n",
    "        return fake, real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Engine(evaluation_step)\n",
    "fid_metric.attach(evaluator, \"fid\")\n",
    "is_metric.attach(evaluator, \"is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid_values = []\n",
    "is_values = []\n",
    "\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(engine):\n",
    "    evaluator.run(dataloader_test,max_epochs=1)\n",
    "    metrics = evaluator.state.metrics\n",
    "    fid_score = metrics['fid']\n",
    "    is_score = metrics['is']\n",
    "    fid_values.append(fid_score)\n",
    "    is_values.append(is_score)\n",
    "    print(f\"Epoch [{engine.state.epoch}/{epochs}] Metric Scores\")\n",
    "    print(f\"*   FID : {fid_score:4f}\")\n",
    "    print(f\"*    IS : {is_score:4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def save_weights(engine):\n",
    "    torch.save(gen.state_dict(), '%s/gen_epoch_%d.pth' % (path_to_weights, engine.state.epoch))\n",
    "    torch.save(dis.state_dict(), '%s/dis_epoch_%d.pth' % (path_to_weights, engine.state.epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.metrics import RunningAverage\n",
    "\n",
    "\n",
    "RunningAverage(output_transform=lambda x: x[\"Loss_G\"]).attach(trainer, 'Loss_G')\n",
    "RunningAverage(output_transform=lambda x: x[\"Loss_D\"]).attach(trainer, 'Loss_D')\n",
    "RunningAverage(output_transform=lambda x: x[\"D_x\"]).attach(trainer, 'D_x')\n",
    "RunningAverage(output_transform=lambda x: x[\"D_G_z1\"]).attach(trainer, 'D_G_z1')\n",
    "RunningAverage(output_transform=lambda x: x[\"D_G_z2\"]).attach(trainer, 'D_G_z2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.contrib.handlers import ProgressBar\n",
    "\n",
    "\n",
    "ProgressBar().attach(trainer, metric_names=['Loss_G','Loss_D',\"D_x\",\"D_G_z1\",\"D_G_z2\"])\n",
    "ProgressBar().attach(evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(*args):\n",
    "    trainer.run(dataloader_train, max_epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with idist.Parallel(backend='gloo') as parallel:\n",
    "    parallel.run(training)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computer-vision-main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
